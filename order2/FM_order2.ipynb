{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565039d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library import\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "from io import StringIO\n",
    "import shutil\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "from TMM_cal_TRC_substrate import TMM_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "#   np.random.seed() \n",
    "    shuffled_indices = np.random.permutation(len(data)) \n",
    "    \n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    \n",
    "    test_indices = np.sort(test_indices, axis=0)\n",
    "    train_indices = np.sort(train_indices, axis=0) \n",
    "    return train_indices, test_indices\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "    def __init__(self, n=None, k=None):\n",
    "        super().__init__()\n",
    "        self.V = nn.Parameter(torch.randn(n, k),requires_grad=True) \n",
    "        self.lin = nn.Linear(n, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True) #S_1^2 Squares of Sum\n",
    "        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True) # S_2 Sum of Squares\n",
    "        \n",
    "        out_inter = 0.5*(out_1 - out_2)\n",
    "        out_lin = self.lin(x)\n",
    "        out = out_inter + out_lin\n",
    "        \n",
    "        return out\n",
    "\n",
    "# for reproducibility\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "####layer = num of bits\n",
    "###filename = dataset name\n",
    "def  train_valid_order2(sample_train, sample_valid, filename, iteration, model_class=None, model_params=None, num_epochs=1, \n",
    "                        early_stop=2000, early_stop_loss = 1, criterion=None, optimizer_class=None, opt_params=None, device=None, file_list=None):\n",
    "    sample_x_train = sample_train #batch train\n",
    "    sample_y_valid = sample_valid #batch valid\n",
    "    \n",
    "    #number_of_input = sample_train.shape[1]\n",
    "    model = model_class(**model_params).to(device)\n",
    "    optimizer_model = optimizer_class(model.parameters(), **opt_params) # torch.optim.Adam\n",
    "    \n",
    "    min_loss_val = float('inf')\n",
    "    min_epoch = 0\n",
    "    early_stop_count = 0 # if this count == 2000, stop training\n",
    "    best_model = None\n",
    "    validation_interval = len(sample_x_train)//1\n",
    "    \n",
    "    train_time_data = []\n",
    "    evaluation_time_data = []\n",
    "    train_loss_data = []\n",
    "    valid_loss_data = []\n",
    "    \n",
    "    #start_train_time\n",
    "    start_train_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        min_epoch += 1\n",
    "        model.train()\n",
    "        for i,(batch_x, batch_y) in enumerate(sample_x_train):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            FOM_predict = model(batch_x)\n",
    "            loss = criterion(FOM_predict, batch_y)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_model.step()\n",
    "            \n",
    "        # Validate after the end of epochs\n",
    "        if (epoch+1) % validation_interval == 0 or epoch == num_epochs-1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                total_val_loss = 0\n",
    "                total_val_sample = 0\n",
    "                for j, (valid_batch_x, valid_batch_y) in enumerate(sample_y_valid):\n",
    "                    valid_batch_x, valid_batch_y = valid_batch_x.to(device), valid_batch_y.to(device)\n",
    "                    FOM_validate = model(valid_batch_x)\n",
    "                    loss_validate = criterion(FOM_validate, valid_batch_y)\n",
    "                    total_val_loss += loss_validate.item() * valid_batch_x.shape[0]\n",
    "                    total_val_sample += valid_batch_x.shape[0]\n",
    "                val_loss = total_val_loss / total_val_sample\n",
    "                \n",
    "            #Save the model if loss of validation is smaller than minimum\n",
    "            if val_loss < min_loss_val:\n",
    "                min_loss_val = val_loss\n",
    "                early_stop_count = 0\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(best_model, filename+'_k'+str(0)+\"_model.pt\")\n",
    "            else:\n",
    "                if(val_loss < early_stop_loss):\n",
    "                    # early_stop_loss is the absolute criterion of early stop\n",
    "                    # to get a model which have a sufficiently small loss\n",
    "                    early_stop_count += 1\n",
    "                \n",
    "                if early_stop_count >= early_stop:\n",
    "                    print(\"End of epochs\")\n",
    "                    print(f'epoch {epoch+1} train loss: {loss} valid loss {min_loss_val}')\n",
    "                    break\n",
    "    end_train_time = time.time()\n",
    "    train_time_data.append(end_train_time - start_train_time)\n",
    "    #end_train_time\n",
    "    add_list_data(file_list[0],train_time_data)\n",
    "    print(\"epoch : \"+str(min_epoch))\n",
    "    print(\"validation_loss : \"+str(min_loss_val)+)\n",
    "    #  After the end of the last epoch, save the model. \n",
    "    if best_model is None:  \n",
    "        torch.save(model.state_dict(), filename+'_k'+str(0)+\"_model.pt\")\n",
    "        pass\n",
    "    model.load_state_dict(torch.load(filename+'_k'+str(0)+\"_model.pt\",map_location=device))\n",
    "    # save FM hyperparameters\n",
    "    #print('save coefficients')\n",
    "    bias = model.lin.bias.detach().cpu().numpy()\n",
    "    linear = model.lin.weight.detach().cpu().numpy()\n",
    "    linear = linear.reshape(-1,1)\n",
    "    quadratic = model.V.detach().cpu().numpy()\n",
    "    #third_interaction = model.K_third.detach().cpu().numpy()\n",
    "    \n",
    "    bias = np.array(bias)\n",
    "    linear = np.array(linear)\n",
    "    quadratic = np.array(quadratic) \n",
    "    Q_layer=quadratic@np.transpose(quadratic)\n",
    "    \n",
    "    QUBO=np.zeros((linear.shape[0],linear.shape[0])) ## 32 x 32\n",
    "    \n",
    "    QUBO[0:Q_layer.shape[0],0:Q_layer.shape[0]]=Q_layer\n",
    "\n",
    "    for i in range(linear.shape[0]):\n",
    "        for j in range(linear.shape[0]):\n",
    "            if i>=j:\n",
    "                QUBO[i,j]=0\n",
    "\n",
    "    for i in range(linear.shape[0]):\n",
    "        QUBO[i,i]=linear[i][0]\n",
    "        \n",
    "    np.savetxt('bias' + '.txt', bias, fmt='%f')\n",
    "    np.savetxt('QUBO' + '.txt', QUBO, fmt='%f')\n",
    "    \n",
    "    return best_model, min_loss_val\n",
    "\n",
    "\n",
    "def mfi(name_file):\n",
    "    text_file_buffer=open(name_file,'r')\n",
    "    content_buffer=text_file_buffer.read()\n",
    "    np_buffer=StringIO(content_buffer)\n",
    "    data_array=np.loadtxt(np_buffer)\n",
    "    return data_array\n",
    "\n",
    "def save_batch_to_txt(filename, layer, subfolder_name, sub_subfolder_name, batch):\n",
    "    # Create an empty list\n",
    "    results = []\n",
    "\n",
    "    for batch_x, batch_y in batch:\n",
    "        # Convert input data to numpy array and cast data type to int\n",
    "        batch_x = batch_x.numpy().astype(int)\n",
    "        # Convert label data to numpy array and reshape into a column vector\n",
    "        batch_y = batch_y.numpy().reshape(-1, 1)\n",
    "        # Append to the results list\n",
    "        results.append(np.concatenate((batch_y, batch_x), axis=1))\n",
    "\n",
    "    # Create the overall result array\n",
    "    result_np = np.concatenate(results, axis=0)\n",
    "\n",
    "    np.savetxt(f\"tmp{filename}.txt\", result_np, delimiter='\\t')\n",
    "    \n",
    "    # Load the saved file\n",
    "    ar_r = np.loadtxt(\"tmp\"+filename+'.txt')\n",
    "\n",
    "    first_col_cv = ar_r[:, 0].reshape(-1, 1)\n",
    "    rest_cols_cv = ar_r[:, 1:]\n",
    "\n",
    "    # Concatenate first column and the rest columns\n",
    "    result_cv = np.concatenate((first_col_cv, rest_cols_cv), axis=1)\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    save_path = os.path.join(current_directory, subfolder_name, sub_subfolder_name) # Create subfolders if they don't exist\n",
    "    if os.path.exists(save_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    file_path = os.path.join(save_path, f\"{filename}.txt\")\n",
    "\n",
    "    # Create format list\n",
    "    fmt = ['%f'] + ['%d']*layer\n",
    "    with open(file_path, 'a') as file:\n",
    "        for line in result_cv:\n",
    "            file.write(f\"{line}\\n\")\n",
    "\n",
    "    \n",
    "# Load saved bias from a txt file.\n",
    "def get_bias_from_txt(filename):\n",
    "    file_path = filename + '.txt'\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            # 파일 내용을 읽어와 변수에 저장합니다.\n",
    "            bias = float(file.read())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Can not find {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while reading the file: {e}\")\n",
    "    return bias\n",
    "\n",
    "# Load saved QUBO from a txt file.\n",
    "def get_matrix_from_txt(filename):\n",
    "    file_path = filename + '.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        values = line.strip().split()  \n",
    "        row = [float(value) for value in values]  \n",
    "        data.append(row)\n",
    "\n",
    "    QUBO = np.array(data)\n",
    "    return QUBO\n",
    "\n",
    "# Convert index into bits\n",
    "def int_to_padded_float32_binary_array(num, num_bits):\n",
    "    if num < 0:\n",
    "        raise ValueError(\"We cannot process negative numbers.\")\n",
    "    \n",
    "    binary_string = bin(num)[2:]  \n",
    "    \n",
    "    binary_string = binary_string.zfill(num_bits)\n",
    "    \n",
    "    binary_array = np.array(list(binary_string), dtype=np.float32)\n",
    "    \n",
    "    return binary_array\n",
    "\n",
    "\n",
    "# calculate third interaction term\n",
    "def cal_3_interaction(x, third_interactions):\n",
    "    # x = binary bit. np.array\n",
    "    # Third-order terms \"Lemma3, Polynomial Networks and Factorization Machines\" \n",
    "    #homogenius polynomial kernel H3(K_second, x)\n",
    "    # Convert input x and third_interaction into tensors using PyTorch.\n",
    "    x_2_dimension = x.reshape(1, -1)\n",
    "    tensor_x = torch.from_numpy(x_2_dimension)\n",
    "    tensor_k = torch.from_numpy(third_interactions)\n",
    "    H_3_px = torch.matmul(tensor_x, tensor_k).pow(3).sum(1, keepdim=True) \n",
    "    #D3(K_third, x)\n",
    "    D_3_px = torch.matmul(tensor_x.pow(3), tensor_k.pow(3)).sum(1, keepdim=True)        \n",
    "    #D2,1(K_third, x)\n",
    "    D_21_px_D2 = torch.matmul(tensor_x.pow(2), tensor_k.pow(2))\n",
    "    D_21_px_D1 = torch.matmul(tensor_x.pow(1), tensor_k.pow(1))\n",
    "    D_21_px=torch.mul(D_21_px_D2,D_21_px_D1).sum(1, keepdim=True) \n",
    "    # (1/6)*(H3-3D21+2D3)\n",
    "    Anova_3rd = (1/6)*(H_3_px - 3*D_21_px + 2*D_3_px) \n",
    "    return Anova_3rd\n",
    "  \n",
    "\n",
    "# Function to determine if the obtained position exists in the dataset, and if so, to retrieve another random position.\n",
    "def get_random_position(dataset,position):\n",
    "    if any(np.all(dataset[i] == position) for i in range(dataset.shape[0])):\n",
    "        #If the position already exists in the dataset, receive a random value\n",
    "        random_value = random.randint(0,2**(dataset.shape[1])-1)\n",
    "        position = int_to_padded_float32_binary_array(random_value, dataset.shape[1])\n",
    "        return get_random_position(dataset,position)\n",
    "    else:\n",
    "        return position\n",
    "\n",
    "# Add new data to a txt file.\n",
    "def add_data(filename, value):\n",
    "    if type(value) != str:\n",
    "        value_str = str(value)\n",
    "    else:\n",
    "        value_str = value\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename +'.txt', 'r') as file:\n",
    "            existing_data = file.readlines() \n",
    "        with open(filename +'.txt', 'w') as file:\n",
    "            for line in existing_data:\n",
    "                file.write(line)\n",
    "                file.write(f\"{value_str}\\n\")\n",
    "    else:\n",
    "        with open(filename +'.txt', 'a') as file:\n",
    "            file.write(value_str + \"\\n\")\n",
    "            \n",
    "# Add new FOM to a txt file.    \n",
    "def add_FOM_data(filename, value_data, position_data):\n",
    "    if type(value_data) != str:\n",
    "        value_str = str(value_data)\n",
    "        position_str = ' '.join(map(str,position_data))\n",
    "    else:\n",
    "        value_str = value_data\n",
    "        position_str = ' '.join(map(str,position_data))\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename +'.txt', 'r') as file:\n",
    "            existing_data = file.readlines()\n",
    "        with open(filename +'.txt', 'w') as file:\n",
    "            for line in existing_data:\n",
    "                file.write(line)\n",
    "            file.write(f\"{value_str} {position_str}\\n\")\n",
    "    else:\n",
    "        with open(filename +'.txt', 'a') as file:\n",
    "            file.write(f\"{value_str} {position_str}\\n\")\n",
    "\n",
    "def add_list_data(filename, list_data):\n",
    "    all_strings = all(isinstance(item, str) for item in list_data)\n",
    "    str_list = list_data\n",
    "    if not all_strings:\n",
    "        str_list = ', '.join(map(str, list_data)) \n",
    "    \n",
    "    if os.path.isfile(filename + '.txt'):  \n",
    "        with open(filename + '.txt', 'r') as file:\n",
    "            existing_data = file.readlines()\n",
    "        \n",
    "        with open(filename + '.txt', 'w') as file:\n",
    "            for line in existing_data:\n",
    "                file.write(line)\n",
    "            file.write(f\"{str_list}\\n\") \n",
    "    else:\n",
    "        with open(filename + '.txt', 'a') as file:\n",
    "            file.write(str_list + \"\\n\")\n",
    "\n",
    "def save_array_txt_int(file_name, subfolder_name, sub_subfolder_name, array):\n",
    "    # Set up subfolder paths\n",
    "    subfolder_path = os.path.join(os.getcwd(), subfolder_name)\n",
    "    sub_subfolder_path = os.path.join(subfolder_path, sub_subfolder_name)\n",
    "\n",
    "    # Create subfolders if they don't exist\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    # Save data to a temporary file first\n",
    "    if not os.path.exists(sub_subfolder_path):\n",
    "        os.makedirs(sub_subfolder_path)\n",
    "\n",
    "    # Open the file, remove decimals, and write to a new file\n",
    "    temp_file = 'temp_data.txt'\n",
    "    np.savetxt(temp_file, array, delimiter=' ')\n",
    "\n",
    "    # 파일 열어서 소수점 제거 후 새 파일에 쓰기\n",
    "    with open(temp_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove decimals and write to the new file\n",
    "    with open(os.path.join(sub_subfolder_path, file_name + '.txt'), 'w') as new_file:\n",
    "        for line in lines:\n",
    "            data = line.strip().split()  # Split each line by space\n",
    "            integer_data = [str(int(float(value))) for value in data]  # Convert each value to float, then to integer, and store as a string\n",
    "            new_line = ' '.join(integer_data)  # Join each value with space to create a new line\n",
    "            new_file.write(new_line + '\\n')  # Write the new line to the file\n",
    "\n",
    "    # 임시 파일 삭제\n",
    "    os.remove(temp_file)\n",
    "            \n",
    "def save_matrix_to_subfolder(subfolder_name, subsubfolder_name, filename):\n",
    "    current_directory = os.getcwd()\n",
    "    from_file_path = os.path.join(current_directory,filename)\n",
    "    to_file_path = os.path.join(current_directory, subfolder_name, subsubfolder_name, filename)\n",
    "    \n",
    "    shutil.copy(from_file_path, to_file_path)\n",
    "\n",
    "def save_data_to_subfolder(data, subfolder_name, subsubfolder_name, filename):\n",
    "    current_directory = os.getcwd()\n",
    "    subfolder_path = os.path.join(current_directory, subfolder_name, subsubfolder_name)\n",
    "\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    file_path = os.path.join(subfolder_path, f\"{filename}.txt\") \n",
    "\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(str(data)) \n",
    "        \n",
    "def rms_loss(y,y_star):\n",
    "    diffy=y-y_star\n",
    "    rms=np.sqrt(np.mean(diffy**2))\n",
    "    return rms\n",
    "\n",
    "# get predicted FOM from model\n",
    "def model_output(sample_x, model_class, model_params, device, filename):\n",
    "    \n",
    "    #Convert to Torch\n",
    "    if torch.cuda.is_available():   \n",
    "        sample_x_ts=torch.cuda.FloatTensor(sample_x).to(device)\n",
    "        \n",
    "    else: \n",
    "        sample_x_ts=torch.FloatTensor(sample_x).to(device)\n",
    "    y_predict_container=torch.FloatTensor(torch.zeros([sample_x_ts.shape[0],1])).to(device)\n",
    "    \n",
    "    \n",
    "    model = model_class(**model_params).to(device)\n",
    "    model.load_state_dict(torch.load(filename+'_k'+str(0)+\"_model.pt\",map_location=device))\n",
    "    \n",
    "    model.eval()\n",
    "    y_predict_container[:,0]=model(sample_x_ts).reshape(1,-1)\n",
    "    \n",
    "    del model\n",
    "    \n",
    "    y_predict_avg=y_predict_container.mean(dim=1)\n",
    "    return y_predict_avg\n",
    "\n",
    "\n",
    "def save_fom_position_batch(data_values, binary_values, subfolder_name, subsubfolder_name, file_name):\n",
    "    current_directory = os.getcwd()\n",
    "    subfolder_path = os.path.join(current_directory, subfolder_name, subsubfolder_name)\n",
    "\n",
    "    file_path = os.path.join(subfolder_path, f\"{file_name}.txt\")  # 파일 이름 설정\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        for i in range(len(data_values)):\n",
    "            line = f\"{data_values[i]} {binary_values[i]}\"\n",
    "            file.write(f\"{line}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70badb77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#main\n",
    "\n",
    "# Generating a random seed\n",
    "seed_everything()\n",
    "\n",
    "# data txt files for plot\n",
    "# save time data\n",
    "total_time_data = '1.total_time'\n",
    "# save training time data\n",
    "training_time_data = '2.training_time'\n",
    "# save evaluation time data\n",
    "evaluation_time_data = '3.evaluation_time'\n",
    "# save brute force time data\n",
    "Brute_time_data = '4.brute_force_time'\n",
    "# save whether the model found or random position found in txt file.\n",
    "random_find = '5.Random_find' # If Random finds, 'o'. Else, 'x'.\n",
    "# save the calculated Figure of Merit (FOM) values by the model.\n",
    "model_preds = '6.Model_predict_value'\n",
    "# save the values of train loss and validation loss.\n",
    "train_loss_data = '7.Train_loss_value'\n",
    "valid_loss_data = '8.Valid_loss_value'\n",
    "\n",
    "train_set_data = '10.train_data_iter'\n",
    "valid_set_data = '11.valid_data_iter'\n",
    "\n",
    "txt_list = [training_time_data, evaluation_time_data, train_loss_data, valid_loss_data, train_set_data, valid_set_data]\n",
    "\n",
    "#------------------------ you should control this parameter------------------------#\n",
    "#          dataset_name and Hyper parameters                 #\n",
    "# dataset\n",
    "dataset_name = 'training_data_FM_benchmark_1_20'\n",
    "###\n",
    "train_ = pd.read_csv(dataset_name + '.txt', sep=' ', header=None)# 총 25개의 데이터\n",
    "train_ = train_.values.astype(np.float32)\n",
    "X_input_ = train_[:,1:] ## bit. 총 25개의 데이터\n",
    "n = X_input_.shape[1]\n",
    "###\n",
    "\n",
    "#------------------------ Hyper parameter -----------------------------#\n",
    "iterations = 3000\n",
    "\n",
    "Model_class = TorchFM\n",
    "Model_params={'n':n, 'k' : 8}\n",
    "Num_epochs = 10000    \n",
    "Early_stop = 2000\n",
    "Early_stop_loss = 2.5\n",
    "Criterion = nn.MSELoss()\n",
    "Optimizer_class = torch.optim.Adam\n",
    "Opt_params={'lr': 0.001, 'weight_decay' : 0}\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "option1 = 2 # The number of substance. In this problems, SiO2 and TiO2\n",
    "option2 = n # Exponent. If option2 == n (bit), only 1 junk for brute force. If n-1, two junk.\n",
    "\n",
    "sampling_size = 200 # If the dataset exceeds 200 entries, increment the data gradually.\n",
    "#if you don't want to sample the dataset, use next line instead of above line\n",
    "#sampling_size = iteration + X_input_.shape[0] + 1\n",
    "slope = 2 # The gradient of increaseing data. 1/2, 1/4 ...\n",
    "###\n",
    "\n",
    "batch_size_train = 2**10   \n",
    "batch_size_validation = 2**8 # Because validation set is 1/4 of Train set \n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    start_total_time = time.time()\n",
    "    train = pd.read_csv(dataset_name + '.txt', sep=' ', header=None)# start from 25 datas\n",
    "    train = train.values.astype(np.float32)\n",
    "    All_X_input = train[:,1:]\n",
    "    \n",
    "    #sampling\n",
    "    NOS = 0\n",
    "    if train.shape[0] > sampling_size:\n",
    "        NOS = sampling_size + (train.shape[0] - sampling_size)//slope\n",
    "    else:\n",
    "        NOS = train.shape[0]\n",
    "    \n",
    "    random_indices = np.random.choice(train.shape[0], size=NOS, replace=False)\n",
    "    sampled_dataset = train[random_indices]\n",
    "    \n",
    "    X_input = sampled_dataset[:,1:] ## Binary bit\n",
    "    train_target = sampled_dataset[:,0] ## fom\n",
    "\n",
    "    train_indices, test_indices = split_train_test(X_input, 0.2) ## test set이 0.2, train set이 0.8\n",
    "\n",
    "    X_train_num = int(train_indices.shape[0]) \n",
    "    X_test_num = int(test_indices.shape[0]) \n",
    "\n",
    "    X_train = X_input[train_indices] # Convert to bits at the specified index.\n",
    "    X_test = X_input[test_indices]\n",
    "\n",
    "    target = train_target[train_indices] # FOM of train dataset\n",
    "    real_y = train_target[test_indices] # FOM of validation dataset\n",
    "\n",
    "    y = target.astype(np.float32)\n",
    "    y = y.reshape(-1,1)\n",
    "    \n",
    "    val_y = real_y.astype(np.float32)\n",
    "    val_y = val_y.reshape(-1,1)\n",
    "    \n",
    "    n = X_train.shape[1] # number of total binary vector\n",
    "\n",
    "    sample_binary_train_ts = torch.from_numpy(X_train).float()\n",
    "    sample_fom_train_ts = torch.from_numpy(y) # FOM of train dataset\n",
    "    sample_binary_test_ts = torch.from_numpy(X_test).float()\n",
    "    sample_fom_test_ts = torch.from_numpy(val_y) # FOM of validation dataset\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(sample_binary_train_ts, sample_fom_train_ts)\n",
    "    val_dataset = torch.utils.data.TensorDataset(sample_binary_test_ts, sample_fom_test_ts)\n",
    "    \n",
    "    if len(train_dataset)<= batch_size_train :\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=False, drop_last=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size_validation, shuffle=False, drop_last=False)\n",
    "    else :\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=False, drop_last=True)\n",
    "        \n",
    "        if len(val_dataset) <=  batch_size_validation:\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size_validation, shuffle=False, drop_last=False)\n",
    "        else:    \n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size_validation, shuffle=False, drop_last=True)\n",
    "    \n",
    "    save_batch_to_txt(train_set_data, n, 'dataset', str(i), train_loader) #train_filename.txt file\n",
    "    save_batch_to_txt(valid_set_data, n, 'dataset', str(i), val_loader) #cv_filename.txt file\n",
    "    \n",
    "    Best_model, Min_loss_val = train_valid_order2(sample_train = train_loader,\n",
    "                                                  sample_valid = val_loader,\n",
    "                                                  filename = dataset_name,\n",
    "                                                  iteration = i,\n",
    "                                                  model_class = Model_class, \n",
    "                                                  model_params=Model_params,\n",
    "                                                 num_epochs = Num_epochs,\n",
    "                                                 early_stop = Early_stop,\n",
    "                                                  early_stop_loss = Early_stop_loss,\n",
    "                                                 criterion = Criterion,\n",
    "                                                 optimizer_class=Optimizer_class,\n",
    "                                                 opt_params=Opt_params,\n",
    "                                                 device=DEVICE,\n",
    "                                                 file_list=txt_list)\n",
    "    \n",
    "    #Brute Search Digital Space\n",
    "    qv_vector_size=int(X_train.shape[1]) # Length of vector.\n",
    "    junk_number=int((option1**qv_vector_size)/(option1**option2)) \n",
    "    x_min_junk_container=np.zeros([junk_number,qv_vector_size])\n",
    "    fom_min_junk_container=np.zeros([junk_number]) # Space to store FOM\n",
    "    \n",
    "    #Brute force\n",
    "    start_brute_time = time.time()\n",
    "    \n",
    "    for idx in range(junk_number):   \n",
    "\n",
    "        with open('Lv'+str(option1)+'_W'+str(qv_vector_size)+'_cJ'+str(idx+1)+'_tJ'+str(junk_number)+'.npy', 'rb') as f:\n",
    "            grid_array=np.load(f) \n",
    "            current_x_junk=np.float32(grid_array) \n",
    "            Y_pred_array=model_output(current_x_junk,Model_class, Model_params, DEVICE, dataset_name)\n",
    "            x_initial_guess=current_x_junk[Y_pred_array.argmin(),:]\n",
    "            x_min_junk_container[idx,:]=x_initial_guess \n",
    "            fom_min_junk_container[idx]=Y_pred_array.min()\n",
    "\n",
    "            numpy_Y_pred_array = Y_pred_array.detach().cpu().numpy()\n",
    "            list_Y_pred=numpy_Y_pred_array.tolist()\n",
    "            with open('Y_pred'+dataset_name+'.txt','a') as file: \n",
    "                for item in list_Y_pred:\n",
    "                    file.write(str(item) + '\\n')\n",
    "            \n",
    "    x_optimal = x_min_junk_container[fom_min_junk_container.argmin(),:]\n",
    "    value_ = fom_min_junk_container.argmin()\n",
    "    \n",
    "    position = get_random_position(All_X_input, x_optimal)\n",
    "    \n",
    "    end_brute_time = time.time()\n",
    "    add_data(Brute_time_data, end_brute_time - start_brute_time)\n",
    "    \n",
    "    \n",
    "    BIT = X_input.shape[1] \n",
    "    bias = get_bias_from_txt('bias')\n",
    "    QUBO = get_matrix_from_txt('QUBO')\n",
    "    \n",
    "    save_matrix_to_subfolder('dataset', str(i),\"bias.txt\")\n",
    "    save_matrix_to_subfolder('dataset', str(i),\"QUBO.txt\")\n",
    "    \n",
    "    if np.array_equal(x_optimal, position):\n",
    "        print(f\"Position from model: {position}\")\n",
    "        add_data(random_find,'x')\n",
    "        x = position\n",
    "        value_ = bias + x@QUBO@np.transpose(x)# + cal_3_interaction(x, k_third)\n",
    "    else:\n",
    "        print(f\"Position from randomness : {position}\")\n",
    "        add_data(random_find,'o')\n",
    "        x = position\n",
    "        value_ = bias + x@QUBO@np.transpose(x)# + cal_3_interaction(x, k_third)\n",
    "    \n",
    "    add_FOM_data(model_preds, value_, position)  \n",
    "    ### Calculate rms from train, validation\n",
    "    tr_y_predict = model_output(X_train, Model_class, Model_params, DEVICE, dataset_name)\n",
    "    # Receives a binary vector and its corresponding file name, \n",
    "    # then assigns the FOM value to tr_y_predict.\n",
    "    tr_y_predict=tr_y_predict.cpu()\n",
    "    tr_y_predict=tr_y_predict.detach().numpy()\n",
    "    \n",
    "    ### save data (predicted value, position)\n",
    "    save_fom_position_batch(tr_y_predict, X_train,'dataset', str(i), train_set_data)\n",
    "    \n",
    "    # Retrieve a tensor variable and copy it to the CPU.\n",
    "    tr_rms=rms_loss(y,tr_y_predict) # y represents the FOM values of the train dataset.\n",
    "    save_data_to_subfolder(tr_rms, 'dataset', str(i),train_loss_data)\n",
    "    \n",
    "    cv_y_predict = model_output(X_test, Model_class, Model_params, DEVICE, dataset_name)\n",
    "    cv_y_predict=cv_y_predict.cpu()\n",
    "    cv_y_predict=cv_y_predict.detach().numpy()\n",
    "    \n",
    "    ### save data (predicted value, position)\n",
    "    save_fom_position_batch(cv_y_predict, X_test,'dataset', str(i), valid_set_data)\n",
    "    \n",
    "    cv_rms=rms_loss(val_y,cv_y_predict)\n",
    "    \n",
    "    save_data_to_subfolder(cv_rms, 'dataset', str(i),valid_loss_data)\n",
    "    \n",
    "    # Calculate real FOM \n",
    "    FOM = TMM_cal(position)\n",
    "    print(f\"FOM : {FOM}\" + '\\n')\n",
    "    \n",
    "    # save (FOM, position) data to dataset.txt \n",
    "    add_FOM_data(dataset_name, FOM, position)\n",
    "    os.remove(dataset_name+'_k'+str(0)+\"_model.pt\")\n",
    "    end_total_time = time.time()\n",
    "    add_data(total_time_data, end_total_time - start_total_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
